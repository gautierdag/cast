{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "docci = load_dataset(\"google/docci\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return clip_processor(images=[b[\"image\"] for b in batch], return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    docci[\"train\"],\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "features_tensors = []\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    batch_device = {k: v.to(clip_model.device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**batch_device)\n",
    "        features_tensors.append(image_features)\n",
    "\n",
    "features_tensors = torch.cat(features_tensors)\n",
    "out = features_tensors.cpu()\n",
    "# normalize the image features\n",
    "out = F.normalize(out, p=2, dim=1)\n",
    "torch.save(out, \"feats/docci_train_img_clip_features.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "def collate_fn_text(batch):\n",
    "    return clip_processor(\n",
    "        text=[b[\"description\"] for b in batch],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    docci[\"train\"],\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_text,\n",
    ")\n",
    "features_tensors = []\n",
    "for batch in tqdm(dataloader):\n",
    "    batch_device = {k: v.to(clip_model.device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.get_text_features(\n",
    "            input_ids=batch_device[\"input_ids\"]\n",
    "        )\n",
    "        features_tensors.append(text_features)\n",
    "\n",
    "features_tensors = torch.cat(features_tensors)\n",
    "out = features_tensors.cpu()\n",
    "# normalize the image features\n",
    "out = F.normalize(out, p=2, dim=1)\n",
    "torch.save(out, \"feats/docci_train_text_clip_features.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load features from the dataset and sample example pairs from docci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# get lengths of descriptions to filter short ones\n",
    "# NOTE: we prefer long descriptions because they are more likely to contain interesting scenes\n",
    "lengths = []\n",
    "for i in tqdm(range(len(docci[\"train\"]))):\n",
    "    lengths.append(len(docci[\"train\"][i][\"description\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "out = torch.load(\"feats/docci_train_img_clip_features.pt\")\n",
    "img_sim_matrix = out @ out.T\n",
    "# mask diagonal\n",
    "mask = torch.eye(img_sim_matrix.shape[0], dtype=bool)\n",
    "img_sim_matrix[mask] = 0\n",
    "\n",
    "out = torch.load(\"feats/docci_train_text_clip_features.pt\")\n",
    "text_sim_matrix = out @ out.T\n",
    "# mask diagonal\n",
    "mask = torch.eye(text_sim_matrix.shape[0], dtype=bool)\n",
    "text_sim_matrix[mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seen_idxs = set()\n",
    "\n",
    "pair_dataset = []\n",
    "\n",
    "ranges = [(0.75, 0.8), (0.8, 0.85), (0.85, 0.9), (0.9, 0.95)]\n",
    "\n",
    "N = 1000\n",
    "while len(pair_dataset) < N:\n",
    "    # random_idx = random.randint(0, len(docci[\"train\"]) - 1)\n",
    "    random_idx = random.randint(0, len(docci[\"train\"]) - 1)\n",
    "    random_range = random.choice(ranges)\n",
    "    if random_idx in seen_idxs:\n",
    "        continue\n",
    "\n",
    "    # filter out short descriptions\n",
    "    if lengths[random_idx] < 500:\n",
    "        continue\n",
    "\n",
    "    # find all the indexes that are in the range\n",
    "    indexes = torch.where(\n",
    "        (img_sim_matrix[random_idx] >= random_range[0])\n",
    "        & (img_sim_matrix[random_idx] <= random_range[1])\n",
    "    )[0]\n",
    "    if len(indexes) == 0:\n",
    "        continue\n",
    "\n",
    "    # remove the indexes that have been seen\n",
    "    indexes = list(set(indexes.tolist()) - seen_idxs)\n",
    "\n",
    "    # filter out short descriptions\n",
    "    indexes = [i for i in indexes if lengths[i] > 500]\n",
    "    if len(indexes) == 0:\n",
    "        continue\n",
    "\n",
    "    second_idx = random.choice(indexes)\n",
    "    seen_idxs.add(random_idx)\n",
    "    seen_idxs.add(second_idx)\n",
    "\n",
    "    pair_dataset.append(\n",
    "        (\n",
    "            random_idx,\n",
    "            second_idx,\n",
    "            img_sim_matrix[random_idx][second_idx].item(),\n",
    "            text_sim_matrix[random_idx][second_idx].item(),\n",
    "            f\"{random_range[0]} - {random_range[1]}\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    pair_dataset, columns=[\"idx1\", \"idx2\", \"img_similarity\", \"desc_similarity\", \"range\"]\n",
    ")\n",
    "df[\"id\"] = df[\"idx1\"].astype(str) + \"_\" + df[\"idx2\"].astype(str)\n",
    "\n",
    "# subsample 25 from each range\n",
    "df = (\n",
    "    df.groupby(\"range\")\n",
    "    .apply(lambda x: x.sample(25, replace=False))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df.sort_values(\"img_similarity\", ascending=False, inplace=True)\n",
    "assert df[\"id\"].nunique() == len(df)\n",
    "assert df[\"idx1\"].nunique() == len(df)\n",
    "assert df[\"idx2\"].nunique() == len(df)\n",
    "assert len(df) == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot across the ranges\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(data=df, x=\"img_similarity\", y=\"desc_similarity\", hue=\"range\")\n",
    "plt.title(\"CLIP Image vs Description similarity in selected ranges\")\n",
    "plt.legend(title=\"Img Similarity Bin\")\n",
    "plt.xlabel(\"Image Similarity\")\n",
    "plt.ylabel(\"Description Similarity\")\n",
    "# save plot\n",
    "plt.savefig(\"plots/clip_similarity.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lengths(idx):\n",
    "    return len(docci[\"train\"][idx][\"description\"])\n",
    "\n",
    "\n",
    "df[\"avg_desc_length\"] = (df[\"idx1\"].apply(lengths) + df[\"idx2\"].apply(lengths)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot across the ranges\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(data=df, x=\"avg_desc_length\", y=\"img_similarity\", hue=\"range\")\n",
    "plt.title(\"CLIP Image vs Description length in selected ranges\")\n",
    "plt.legend(title=\"Img Similarity Bin\")\n",
    "plt.xlabel(\"Average Description Length\")\n",
    "plt.ylabel(\"Description Similarity\")\n",
    "# save plot\n",
    "plt.savefig(\"plots/clip_vs_length.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset as list of jsons\n",
    "import json\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    idx1 = row[\"idx1\"]\n",
    "    idx2 = row[\"idx2\"]\n",
    "\n",
    "    image1 = docci[\"train\"][idx1][\"image\"]\n",
    "    image2 = docci[\"train\"][idx2][\"image\"]\n",
    "\n",
    "    # save to data/imgs\n",
    "    image1.save(f\"data/imgs/{i}_0.jpg\")\n",
    "    image2.save(f\"data/imgs/{i}_1.jpg\")\n",
    "    example_dict = {\n",
    "        \"id\": i,\n",
    "        \"docci_idx_0\": idx1,\n",
    "        \"docci_idx_1\": idx2,\n",
    "        \"img_pair_similarity\": row[\"img_similarity\"],\n",
    "        \"desc_pair_similarity\": row[\"desc_similarity\"],\n",
    "        \"description_0\": docci[\"train\"][idx1][\"description\"],\n",
    "        \"description_1\": docci[\"train\"][idx2][\"description\"],\n",
    "    }\n",
    "    with open(f\"data/{i}.json\", \"w\") as f:\n",
    "        json.dump(example_dict, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "consistency",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
